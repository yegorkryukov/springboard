{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yegor/anaconda3/envs/springboard/lib/python3.8/site-packages/pandas_datareader/compat/__init__.py:7: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  from pandas.util.testing import assert_frame_equal\n"
     ]
    }
   ],
   "source": [
    "import pymongo as pm\n",
    "import requests\n",
    "import pandas as pd \n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "# yahoo_fin documentation: https://theautomatic.net/yahoo_fin-documentation/\n",
    "from yahoo_fin import stock_info as si \n",
    "from pandas_datareader import DataReader\n",
    "import numpy as np\n",
    "\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from newspaper import Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/yegor/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/yegor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# need to do once for vader to work\n",
    "nltk.download('vader_lexicon')\n",
    "# need to do once for newspaper to work\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "# check available plotly renderers\n",
    "pio.renderers\n",
    "\n",
    "# set pandas plotting backend to plotly\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "# options for plotly to work in the notebook\n",
    "pio.renderers.default = 'notebook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ticker_data(ticker_data, xaxis_rangeslider_visible=True):\n",
    "    \"\"\"\n",
    "    Plot candelstick\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = go.Figure(data=[go.Candlestick(\n",
    "        x=ticker_data.index,\n",
    "        open=ticker_data.open,\n",
    "        high=ticker_data.high,\n",
    "        low=ticker_data.low,\n",
    "        close=ticker_data.close,\n",
    "#         increasing_line_color= 'blue', \n",
    "#         decreasing_line_color= 'red'\n",
    "    )])\n",
    "\n",
    "    # uncomment below to remove rangeslider\n",
    "    fig.update_layout(xaxis_rangeslider_visible=xaxis_rangeslider_visible)\n",
    "\n",
    "#     fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'MSFT'\n",
    "start_date = '01/01/2015'\n",
    "end_date = '05/31/2020'\n",
    "ticker_data = si.get_data(ticker, start_date=start_date, end_date=end_date)\n",
    "plot_ticker_data(ticker_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MongoDB\n",
    "!brew services start mongodb-community@4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop MongoDB\n",
    "!brew services stop mongodb-community@4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendation(ticker):\n",
    "    \"\"\"\n",
    "    Obtaines yahoo recommendations for a 'ticker'\n",
    "    \"\"\"\n",
    "    lhs_url = 'https://query2.finance.yahoo.com/v10/finance/quoteSummary/'\n",
    "    rhs_url = '?formatted=true&crumb=swg7qs5y9UP&lang=en-US&region=US&' \\\n",
    "              'modules=upgradeDowngradeHistory,recommendationTrend,' \\\n",
    "              'financialData,earningsHistory,earningsTrend,industryTrend&' \\\n",
    "              'corsDomain=finance.yahoo.com'\n",
    "              \n",
    "    url =  lhs_url + ticker + rhs_url\n",
    "    r = requests.get(url)\n",
    "    if not r.ok:\n",
    "        recommendation = 6\n",
    "    try:\n",
    "        result = r.json()['quoteSummary']['result'][0]\n",
    "        recommendation =result['financialData']['recommendationMean']['fmt']\n",
    "    except:\n",
    "        recommendation = 6\n",
    "    \n",
    "    return recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_store_recommendations(ticker, dt=None):\n",
    "    \"\"\"\n",
    "    Retrieves yahoo recommendations for a 'ticker' and stores to MongoDB avoiding duplicates\n",
    "    \"\"\"\n",
    "    client = pm.MongoClient('mongodb://localhost:27017')\n",
    "    collection = client['news']['recommendations']\n",
    "    \n",
    "    day = date.today().strftime('%Y-%m-%d') if dt is None else dt\n",
    "\n",
    "    doc = {\n",
    "        'recommendations': {\n",
    "            'date' : day,\n",
    "            'recommendation' : get_recommendation(ticker)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    collection.update_one(\n",
    "        {'ticker' : ticker},\n",
    "        {'$addToSet': doc},\n",
    "        upsert = True\n",
    "    )\n",
    "    print(f\"Saved {ticker}: {doc}\")\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_store_news(ticker):\n",
    "    \"\"\"\n",
    "    Obtaines news for the 'ticker' and save to MongoDB\n",
    "    \"\"\"\n",
    "    logger.info(f'Working on {ticker}')\n",
    "    client = pm.MongoClient('mongodb://localhost:27017')\n",
    "    collection = client['news']['recommendations']\n",
    "    finwiz_url = 'https://finviz.com/quote.ashx?t='\n",
    "    url = finwiz_url + ticker\n",
    "    req = Request(url=url,headers={'user-agent': 'my-app/0.0.1'}) \n",
    "    response = urlopen(req)    \n",
    "    html = BeautifulSoup(response)\n",
    "    news_table = html.find(id='news-table')\n",
    "\n",
    "    for x in news_table.findAll('tr'):\n",
    "\n",
    "        title = x.a.get_text() \n",
    "        link = x.a['href']\n",
    "        \n",
    "        date_scrape = x.td.text.strip().split()\n",
    "\n",
    "        if len(date_scrape) == 1:\n",
    "            time = date_scrape[0]\n",
    "            \n",
    "        else:\n",
    "            date = date_scrape[0]\n",
    "            time = date_scrape[1]\n",
    "        dt = date + ' ' + time\n",
    "        try:\n",
    "            article = Article(link)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            article.nlp()\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        doc = {\n",
    "            'news' : {\n",
    "                'datetime' : dt,\n",
    "                'url'      : link,\n",
    "                'title'    : title,\n",
    "                'text'     : article.text,\n",
    "                'keywords' : article.keywords,\n",
    "                'summary'  : article.summary\n",
    "            }\n",
    "        }\n",
    "\n",
    "        collection.update_one(\n",
    "            {'ticker' : ticker},\n",
    "            {'$addToSet': doc},\n",
    "            upsert = True\n",
    "        )\n",
    "        \n",
    "        print(f\"Saved {ticker}: {dt}/ {doc['news']['title']}\")\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    logger.info('Launching multi')\n",
    "    tickers = si.tickers_sp500()\n",
    "    \n",
    "    p = Pool(10)\n",
    "    result = p.map_async(get_and_store_news, tickers)\n",
    "    result.get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the SP500 list\n",
    "tickers = si.tickers_sp500()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cpu :  8\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "print(\"Number of cpu : \", multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-16355e716a1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mticker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtickers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#     print(get_and_store_recommendations(ticker))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mget_and_store_news\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-30ce91cb1682>\u001b[0m in \u001b[0;36mget_and_store_news\u001b[0;34m(ticker)\u001b[0m\n\u001b[1;32m     45\u001b[0m         }\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         collection.update_one(\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m'ticker'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m'$addToSet'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'collection' is not defined"
     ]
    }
   ],
   "source": [
    "# retrieve and store current recommendations\n",
    "for ticker in tickers: \n",
    "#     print(get_and_store_recommendations(ticker))\n",
    "    get_and_store_news(ticker, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_and_store_news('AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(tickers, recommendations)), \n",
    "               columns =['company', 'recommendation']) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['recommendation'] = pd.to_numeric(df['recommendation'])\n",
    "df = df[df.recommendation != 6]\n",
    "df.sort_values(by=['recommendation'], ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this analysis, the only values that are of importance are 1–1.5, 3, and 4.5–5. These are the scores, that signify the highest chance for an event to take place, and thus are the best indicators..\n",
    "only specific values are of importance (the highest indicators). As a result, I will be creating three novel dataframes, named “hold_df”, “buy_df”, and “sell_df” that will then be contacted into “new_df”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_df = df[df.recommendation == 3]\n",
    "buy_df = df[df.recommendation <= 1.5]\n",
    "sell_df = df[df.recommendation >= 4.5]\n",
    "\n",
    "df_list = [hold_df, buy_df, sell_df]\n",
    "new_df = pd.concat(df_list)\n",
    "new_df.reset_index(level=0, inplace=True)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to fetch the News and Twitter feed. Once both feeds are successfully fetched, sentiment analysis for each stock will be individually conducted for each platform and then the two results will be added and divided by two.\n",
    "So the final sentiment score will be calculated as follows:\n",
    "Final Score = (Twitter Sentiment Score + News Feed Sentiment Score) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers =[] \n",
    "  \n",
    "for index, rows in new_df.iterrows(): \n",
    "    tickers.append(rows.company) \n",
    "\n",
    "print(tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finviz is going to be used to parse the news data into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finwiz_url = 'https://finviz.com/quote.ashx?t='\n",
    "\n",
    "news_tables = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    url = finwiz_url + ticker\n",
    "    req = Request(url=url,headers={'user-agent': 'my-app/0.0.1'}) \n",
    "    response = urlopen(req)    \n",
    "    html = BeautifulSoup(response)\n",
    "    news_table = html.find(id='news-table')\n",
    "    news_tables[ticker] = news_table\n",
    "    \n",
    "parsed_news = []\n",
    "\n",
    "for file_name, news_table in news_tables.items():\n",
    "    for x in news_table.findAll('tr'):\n",
    "    \n",
    "        text = x.a.get_text() \n",
    "        date_scrape = x.td.text.split()\n",
    "\n",
    "        if len(date_scrape) == 1:\n",
    "            time = date_scrape[0]\n",
    "            \n",
    "        else:\n",
    "            date = date_scrape[0]\n",
    "            time = date_scrape[1]\n",
    "        ticker = file_name.split('_')[0]\n",
    "        \n",
    "        parsed_news.append([ticker, date, time, text])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['ticker', 'date', 'time', 'headline']\n",
    "\n",
    "parsed_and_scored_news = pd.DataFrame(parsed_news, columns=columns)\n",
    "\n",
    "parsed_and_scored_news['date'] = pd.to_datetime(parsed_and_scored_news.date).dt.date\n",
    "\n",
    "parsed_and_scored_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with the data is that in their current form, they can not be used by any model. Thus, I will be grouping the headlines for each company in one string, according to the company they are referring to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_and_scored_news = parsed_and_scored_news.groupby(['ticker'], as_index = False)\\\n",
    "    .agg({'headline': ''.join}, Inplace=True)\n",
    "parsed_and_scored_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "scores = parsed_and_scored_news['headline'].apply(vader.polarity_scores).tolist()\n",
    "\n",
    "scores_df = pd.DataFrame(scores)\n",
    "\n",
    "parsed_and_scored_news = parsed_and_scored_news.join(scores_df, rsuffix='_right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_and_scored_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download language model\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "# for token in doc:\n",
    "#     print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "#             token.shape_, token.is_alpha, token.is_stop)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls stocks_latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset_summary.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('stocks_latest/stock_prices_latest.csv', parse_dates=['date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = ['A']\n",
    "A = df[df.symbol.isin(stock)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.set_index('date', inplace=True)\n",
    "A.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.truncate(before='2015-01-01')['close'].plot(figsize=(16,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_symbol = df.groupby(['date','symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('springboard': conda)",
   "language": "python",
   "name": "python38364bitspringboardconda8eec40d942e84bf0a5a0c76aa15b276b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
